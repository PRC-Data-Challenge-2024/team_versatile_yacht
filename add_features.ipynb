{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, dotenv_values \n",
    "# loading variables from .env file\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aircraft category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "source_data_folder=os.getenv(\"SOURCE_FOLDER\")\n",
    "challenge_file=os.getenv(\"CHALLENGE_FILE\")\n",
    "challenge_file_preproc=os.getenv(\"CHALLENGE_FILE_PREPROC\")\n",
    "submission_file=os.getenv(\"SUBMISSION_FILE\")\n",
    "submission_file_preproc=os.getenv(\"SUBMISSION_FILE_PREPROC\")\n",
    "\n",
    "df = pd.read_csv(os.path.join(source_data_folder,challenge_file))\n",
    "df_submission = pd.read_csv(os.path.join(source_data_folder,submission_file))\n",
    "\n",
    "# Mapping of aircraft_type, and categories\n",
    "weight_class_mapping = {\n",
    "    'A332': 'Heavy', 'A333': 'Heavy', 'A343': 'Heavy',\n",
    "    'A359': 'Heavy', 'B772': 'Heavy', 'B773': 'Heavy',\n",
    "    'B77W': 'Heavy', 'B788': 'Heavy', 'B789': 'Heavy',\n",
    "    'B752': 'Heavy', 'B763': 'Heavy', 'A310': 'Heavy',\n",
    "    'A20N': 'Medium', 'A21N': 'Medium', 'A319': 'Medium', 'A320': 'Medium',\n",
    "    'A321': 'Medium', 'B738': 'Medium', 'B739': 'Medium', 'B38M': 'Medium',\n",
    "    'B39M': 'Medium', 'B737': 'Medium',\n",
    "    'BCS1': 'Light', 'BCS3': 'Light', 'CRJ9': 'Light', 'E190': 'Light',\n",
    "    'E195': 'Light', 'E290': 'Light', 'AT76': 'Light',\n",
    "    'C56X': 'Light'\n",
    "}\n",
    "\n",
    "# Add \"aircraft_category\" column\n",
    "df['aircraft_category'] = df['aircraft_type'].map(weight_class_mapping)\n",
    "df_submission['aircraft_category'] = df_submission['aircraft_type'].map(weight_class_mapping)\n",
    "\n",
    "# Check if all aircraft_type exist\n",
    "df_missing = df[df['aircraft_category'].isna()]\n",
    "if not df_missing.empty:\n",
    "    print(\"The following aircraft_types does not exist in mapping :\")\n",
    "    print(df_missing['aircraft_type'].unique())\n",
    "\n",
    "# Check if all aircraft_type exist\n",
    "df_sub_missing = df_submission[df_submission['aircraft_category'].isna()]\n",
    "if not df_sub_missing.empty:\n",
    "    print(\"The following aircraft_types does not exist in mapping :\")\n",
    "    print(df_sub_missing['aircraft_type'].unique())\n",
    "\n",
    "df.to_csv(os.path.join(source_data_folder,challenge_file_preproc), index=False)\n",
    "df_submission.to_csv(os.path.join(source_data_folder,submission_file_preproc), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load File\n",
    "df_aircraft = pd.read_csv(os.path.join(source_data_folder,os.getenv(\"AIRCRAFT_DB\")))\n",
    "df_test = pd.read_csv(os.path.join(source_data_folder,challenge_file_preproc))\n",
    "df_submission = pd.read_csv(os.path.join(source_data_folder,submission_file_preproc))\n",
    "\n",
    "# Mapping RECAT with aircraft_category\n",
    "recat_mapping = {\n",
    "    \"CAT-A\": \"Heavy\", \n",
    "    \"CAT-B\": \"Heavy\", \n",
    "    \"CAT-C\": \"Heavy\", \n",
    "    \"CAT-D\": \"Medium\", \n",
    "    \"CAT-E\": \"Light\", \n",
    "    \"CAT-F\": \"Light\"\n",
    "}\n",
    "\n",
    "# Update both files with RECAT\n",
    "def update_aircraft_info(df_target, df_aircraft):\n",
    "    for _, row in df_aircraft.iterrows():\n",
    "        aircraft_type = row['Aircraft_Type']\n",
    "        recat_value = row['RECAT-EU']\n",
    "        \n",
    "        # Chech if exists\n",
    "        mask = df_target['aircraft_type'] == aircraft_type\n",
    "        \n",
    "        # If exists, update file\n",
    "        if mask.any():\n",
    "            df_target.loc[mask, 'aircraft_category'] = recat_mapping.get(recat_value, df_target.loc[mask, 'aircraft_category'])\n",
    "            df_target.loc[mask, 'aircraft_OEW'] = row['Aircraft_OEW']\n",
    "            df_target.loc[mask, 'aircraft_ZFW'] = row['Aircraft_ZFW']\n",
    "            df_target.loc[mask, 'aircraft_max_range'] = row['Aircraft_MaxRange']\n",
    "    \n",
    "    return df_target\n",
    "\n",
    "df_test = update_aircraft_info(df_test, df_aircraft)\n",
    "df_submission = update_aircraft_info(df_submission, df_aircraft)\n",
    "\n",
    "# Save changes to file\n",
    "df_test.to_csv(os.path.join(source_data_folder,challenge_file_preproc), index=False)\n",
    "df_submission.to_csv(os.path.join(source_data_folder,submission_file_preproc), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTOW, OEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for CRJ9: Aircraft crj9 not available.\n",
      "Error for BCS3: Aircraft bcs3 not available.\n",
      "Error for AT76: Aircraft at76 not available.\n",
      "Error for BCS1: Aircraft bcs1 not available.\n",
      "Error for A310: Aircraft a310 not available.\n",
      "Error for C56X: Aircraft c56x not available.\n",
      "Error for E290: Aircraft e290 not available.\n",
      "The columns min_tow and max_tow have been added and saved in '/workspaces/PRCDataChallenge/data/challenge_set_preproc.csv'.\n",
      "Error for BCS3: Aircraft bcs3 not available.\n",
      "Error for CRJ9: Aircraft crj9 not available.\n",
      "Error for BCS1: Aircraft bcs1 not available.\n",
      "Error for AT76: Aircraft at76 not available.\n",
      "Error for E290: Aircraft e290 not available.\n",
      "The columns min_tow and max_tow have been added and saved in '/workspaces/PRCDataChallenge/data/submission_set_preproc.csv'.\n"
     ]
    }
   ],
   "source": [
    "from openap import prop\n",
    "import pandas as pd\n",
    "\n",
    "# List of files to process\n",
    "files = [os.path.join(source_data_folder,challenge_file_preproc), os.path.join(source_data_folder,submission_file_preproc)]\n",
    "\n",
    "# Iterate through each file\n",
    "for file in files:\n",
    "    # Load the CSV file\n",
    "    df_flight = pd.read_csv(file)\n",
    "\n",
    "    # List of unique aircraft types in the file\n",
    "    aircraft_types = df_flight['aircraft_type'].unique()\n",
    "\n",
    "    # Initialize a dictionary to store MTOW and OEW values\n",
    "    aircraft_data = {}\n",
    "\n",
    "    # Use the openap library to retrieve information for each aircraft\n",
    "    for aircraft_type in aircraft_types:\n",
    "        try:\n",
    "            # Get aircraft information via ICAO\n",
    "            aircraft = prop.aircraft(f'{aircraft_type}')\n",
    "            mtow = aircraft['mtow']\n",
    "            oew = aircraft['oew']\n",
    "            \n",
    "            # Store results in the dictionary\n",
    "            aircraft_data[aircraft_type] = {'max_tow': mtow, 'min_tow': oew}\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {aircraft_type}: {e}\")\n",
    "            aircraft_data[aircraft_type] = {'max_tow': None, 'min_tow': None}\n",
    "\n",
    "    # Add 'min_tow' and 'max_tow' columns to the DataFrame\n",
    "    df_flight['aircraft_OEW'] = df_flight['aircraft_type'].map(lambda x: aircraft_data.get(x, {}).get('min_tow', None))\n",
    "    df_flight['aircraft_MTOW'] = df_flight['aircraft_type'].map(lambda x: aircraft_data.get(x, {}).get('max_tow', None))\n",
    "\n",
    "    df_flight.to_csv(file, index=False)\n",
    "\n",
    "    print(f\"The columns min_tow and max_tow have been added and saved in '{file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns 'aircraft_OEW' and 'aircraft_MTOW' have been updated for the corresponding aircraft types in '/workspaces/PRCDataChallenge/data/challenge_set_preproc.csv'.\n",
      "The columns 'aircraft_OEW' and 'aircraft_MTOW' have been updated for the corresponding aircraft types in '/workspaces/PRCDataChallenge/data/submission_set_preproc.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of files to process\n",
    "files = [os.path.join(source_data_folder,challenge_file_preproc), os.path.join(source_data_folder,submission_file_preproc)]\n",
    "\n",
    "# Create a dictionary with the values of aircraft_OEW and aircraft_MTOW\n",
    "tow_data = {\n",
    "    'CRJ9': {'aircraft_OEW': 21772, 'aircraft_MTOW': 38329},\n",
    "    'BCS3': {'aircraft_OEW': 37100, 'aircraft_MTOW': 70900},\n",
    "    'AT76': {'aircraft_OEW': 13311, 'aircraft_MTOW': 21000},\n",
    "    'A310': {'aircraft_OEW': 80000, 'aircraft_MTOW': 153000},\n",
    "    'BCS1': {'aircraft_OEW': 37500, 'aircraft_MTOW': 63100},\n",
    "    'C56X': {'aircraft_OEW': 5750, 'aircraft_MTOW': 9163},\n",
    "    'E290': {'aircraft_OEW': 27853, 'aircraft_MTOW': 56400},\n",
    "    'B763': {'aircraft_OEW': 95000, 'aircraft_MTOW': 187000},\n",
    "    'B737': {'aircraft_OEW': 39000, 'aircraft_MTOW': 79000},\n",
    "    'B788': {'aircraft_OEW': 118000, 'aircraft_MTOW': 228000},\n",
    "    'B789': {'aircraft_OEW': 127000, 'aircraft_MTOW': 254000}\n",
    "}\n",
    "\n",
    "# Iterate through each file\n",
    "for file in files:\n",
    "    # Load the CSV file\n",
    "    df_flight = pd.read_csv(file)\n",
    "\n",
    "    # Fill the 'aircraft_OEW' and 'aircraft_MTOW' columns based on aircraft_type\n",
    "    df_flight['aircraft_OEW'] = df_flight.apply(\n",
    "        lambda row: tow_data.get(row['aircraft_type'], {}).get('aircraft_OEW', row.get('aircraft_OEW')),\n",
    "        axis=1\n",
    "    )\n",
    "    df_flight['aircraft_MTOW'] = df_flight.apply(\n",
    "        lambda row: tow_data.get(row['aircraft_type'], {}).get('aircraft_MTOW', row.get('aircraft_MTOW')),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Save the modified file\n",
    "\n",
    "    df_flight.to_csv(file, index=False)\n",
    "\n",
    "    print(f\"The columns 'aircraft_OEW' and 'aircraft_MTOW' have been updated for the corresponding aircraft types in '{file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio TOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'load_ratio' column has been added and saved to '/workspaces/PRCDataChallenge/data/challenge_set_preproc.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "updated_tow_file=os.path.join(source_data_folder,challenge_file_preproc)\n",
    "df_flight = pd.read_csv(updated_tow_file)\n",
    "\n",
    "# Check if the columns 'tow', 'aircraft_OEW', and 'aircraft_MTOW' exist in the DataFrame\n",
    "if all(col in df_flight.columns for col in ['tow', 'aircraft_OEW', 'aircraft_MTOW']):\n",
    "    # Calculate the load_ratio according to the given formula\n",
    "    df_flight['load_ratio'] = (df_flight['tow'] - df_flight['aircraft_OEW']) / (df_flight['aircraft_MTOW'] - df_flight['aircraft_OEW'])\n",
    "    \n",
    "    # Save the modified file\n",
    "    df_flight.to_csv(updated_tow_file, index=False)\n",
    "    \n",
    "    print(\"The 'load_ratio' column has been added and saved to '{}'.\".format(updated_tow_file))\n",
    "else:\n",
    "    print(\"The columns 'tow', 'aircraft_OEW', or 'aircraft_MTOW' are missing in the file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'load_ratio' column has been added and saved to '/workspaces/PRCDataChallenge/data/submission_set_preproc.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "updated_tow_file=os.path.join(source_data_folder,submission_file_preproc)\n",
    "df_flight = pd.read_csv(updated_tow_file)\n",
    "\n",
    "df_flight['load_ratio'] = None\n",
    "\n",
    "df_flight.to_csv(updated_tow_file, index=False)\n",
    "\n",
    "print(\"The 'load_ratio' column has been added and saved to '{}'.\".format(updated_tow_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flown_distance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The modified files have been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_file_preproc=os.path.join(source_data_folder,challenge_file_preproc)\n",
    "submission_file_preproc=os.path.join(source_data_folder,submission_file_preproc)\n",
    "df_test = pd.read_csv(test_file_preproc)\n",
    "df_submission = pd.read_csv(submission_file_preproc)\n",
    "\n",
    "# Function to add the \"ratio_flown_distance_max_range\" column\n",
    "def add_flown_distance_ratio(df_target):\n",
    "    # Assume that 'flown_distance' and 'aircraft_max_range' exist\n",
    "    df_target['ratio_flown_distance_max_range'] = df_target['flown_distance'] / df_target['aircraft_max_range']\n",
    "    return df_target\n",
    "\n",
    "# Add the \"ratio_flown_distance_max_range\" column\n",
    "df_test = add_flown_distance_ratio(df_test)\n",
    "df_submission = add_flown_distance_ratio(df_submission)\n",
    "\n",
    "# Save the modifications to the CSV files\n",
    "df_test.to_csv(test_file_preproc, index=False)\n",
    "df_submission.to_csv(submission_file_preproc, index=False)\n",
    "\n",
    "print(\"\\nThe modified files have been saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode / Max / Median altitude and cruise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The files have been merged and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "altitude_data = pd.read_csv(os.path.join(source_data_folder,os.getenv(\"TRAJECTORY_DATA\")))\n",
    "challenge_set_og = pd.read_csv(test_file_preproc)\n",
    "final_submission_set = pd.read_csv(submission_file_preproc)\n",
    "\n",
    "# Merge altitude_data with challenge_set_og on flight_id\n",
    "merged_challenge_set = pd.merge(\n",
    "    challenge_set_og,\n",
    "    altitude_data[['flight_id','fl_mode','fl_max','fl_median', 'plateau_climb_rate_avg', 'plateau_altitude',\n",
    "                   'ground_airspeed','ground_wind_speed','ground_wind_direction','ground_airspeed_angle',\n",
    "                   'plateau_airspeed','plateau_wind_speed','plateau_wind_direction','plateau_airspeed_angle'\n",
    "                   ]],\n",
    "    on='flight_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge altitude_data with final_submission_set on flight_id\n",
    "merged_final_submission_set = pd.merge(\n",
    "    final_submission_set,\n",
    "    altitude_data[['flight_id','fl_mode','fl_max','fl_median', 'plateau_climb_rate_avg', 'plateau_altitude',\n",
    "                   'ground_airspeed','ground_wind_speed','ground_wind_direction','ground_airspeed_angle',\n",
    "                   'plateau_airspeed','plateau_wind_speed','plateau_wind_direction','plateau_airspeed_angle'\n",
    "                   ]],\n",
    "    on='flight_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save the results to new CSV files\n",
    "merged_challenge_set.to_csv(test_file_preproc, index=False)\n",
    "merged_final_submission_set.to_csv(submission_file_preproc, index=False)\n",
    "\n",
    "print(\"\\nThe files have been merged and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combo Adep / Ades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5393/312833122.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_flight['adep_ades'] = df_flight[['adep', 'ades']].apply(lambda x: f'{x[0]}_{x[1]}', axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file: /workspaces/PRCDataChallenge/data/challenge_set_preproc.csv with new 'adep_ades' column.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5393/312833122.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_flight['adep_ades'] = df_flight[['adep', 'ades']].apply(lambda x: f'{x[0]}_{x[1]}', axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file: /workspaces/PRCDataChallenge/data/submission_set_preproc.csv with new 'adep_ades' column.\n",
      "All files have been updated with the new 'adep_ades' column.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [os.path.join(source_data_folder,challenge_file_preproc), os.path.join(source_data_folder,submission_file_preproc)]\n",
    "\n",
    "# Loop through each file\n",
    "for file in files_to_process:\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df_flight = pd.read_csv(file)\n",
    "\n",
    "    # Create a new column 'adep_ades' that combines 'adep' and 'ades'\n",
    "    df_flight['adep_ades'] = df_flight[['adep', 'ades']].apply(lambda x: f'{x[0]}_{x[1]}', axis=1)\n",
    "\n",
    "    # Assign a unique numeric value to each 'adep_ades' combination\n",
    "    df_flight['adep_ades'] = pd.factorize(df_flight['adep_ades'])[0]\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df_flight.to_csv(file, index=False)\n",
    "\n",
    "    print(f\"Updated file: {file} with new 'adep_ades' column.\")\n",
    "\n",
    "print(\"All files have been updated with the new 'adep_ades' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Circle distance Adep / Ades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file: /workspaces/PRCDataChallenge/data/challenge_set_preproc.csv with great circle distances.\n",
      "Updated file: /workspaces/PRCDataChallenge/data/submission_set_preproc.csv with great circle distances.\n",
      "Great circle distances have been calculated and saved for all files.\n"
     ]
    }
   ],
   "source": [
    "#JGO: To refactor in order to calculate only one time each ADEP/ADES pair: DONE\n",
    "import pandas as pd\n",
    "from traffic.data import airports\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [os.path.join(source_data_folder,challenge_file_preproc), os.path.join(source_data_folder,submission_file_preproc)]\n",
    "\n",
    "# Function to calculate great circle distance\n",
    "def calculate_great_circle_distance(row, adep_latlon, ades_latlon):\n",
    "    adep_coords = adep_latlon.get(row['adep'], None)\n",
    "    ades_coords = ades_latlon.get(row['ades'], None)\n",
    "    if adep_coords and ades_coords:\n",
    "        return great_circle(adep_coords, ades_coords).kilometers\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Loop through each file\n",
    "for file in files_to_process:\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Get unique departure and arrival airport codes\n",
    "    unique_adeps = df['adep'].unique()\n",
    "    unique_ades = df['ades'].unique()\n",
    "\n",
    "    # Create dictionaries to store lat/lon of airports\n",
    "    adep_latlon = {}\n",
    "    ades_latlon = {}\n",
    "\n",
    "    # Retrieve lat/lon for unique departure airports\n",
    "    for adep in unique_adeps:\n",
    "        try:\n",
    "            airport = airports[adep]\n",
    "            adep_latlon[adep] = airport.latlon\n",
    "        except ValueError:\n",
    "            adep_latlon[adep] = None\n",
    "\n",
    "    # Retrieve lat/lon for unique arrival airports\n",
    "    for ades in unique_ades:\n",
    "        try:\n",
    "            airport = airports[ades]\n",
    "            ades_latlon[ades] = airport.latlon\n",
    "        except ValueError:\n",
    "            ades_latlon[ades] = None\n",
    "    tmp=df.groupby([\"adep\",\"ades\"]).count().reset_index()\n",
    "    tmp=tmp[[\"adep\",\"ades\"]]\n",
    "    # Calculate great circle distances\n",
    "    tmp['great_circle_distance_adep_ades']=tmp.apply(calculate_great_circle_distance, axis=1, \n",
    "                                                      args=(adep_latlon, ades_latlon))\n",
    "    df=df.merge(tmp,on=[\"adep\",\"ades\"])\n",
    "    #df['great_circle_distance_adep_ades'] = df.apply(calculate_great_circle_distance, axis=1, \n",
    "    #                                                  args=(adep_latlon, ades_latlon))\n",
    "\n",
    "    # Save the modified DataFrame back to the CSV file\n",
    "    df.to_csv(file, index=False)\n",
    "\n",
    "    print(f\"Updated file: {file} with great circle distances.\")\n",
    "\n",
    "print(\"Great circle distances have been calculated and saved for all files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flown_distance / Great_circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file: /workspaces/PRCDataChallenge/data/challenge_set_preproc.csv with new 'flown_distance_km' and 'great_circle_flown_distance' columns.\n",
      "Updated file: /workspaces/PRCDataChallenge/data/submission_set_preproc.csv with new 'flown_distance_km' and 'great_circle_flown_distance' columns.\n",
      "All files have been updated with the new calculations.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [os.path.join(source_data_folder,challenge_file_preproc), os.path.join(source_data_folder,submission_file_preproc)]\n",
    "\n",
    "# Loop through each file\n",
    "for file in files_to_process:\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Convert flown_distance from nautical miles to kilometers\n",
    "    # 1 nautical mile = 1.852 kilometers\n",
    "    df['flown_distance_km'] = df['flown_distance'] * 1.852\n",
    "\n",
    "    # Calculate the great circle flown distance\n",
    "    df['great_circle_flown_distance'] = df['flown_distance_km'] / df['great_circle_distance_adep_ades']\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(file, index=False)\n",
    "\n",
    "    print(f\"Updated file: {file} with new 'flown_distance_km' and 'great_circle_flown_distance' columns.\")\n",
    "\n",
    "print(\"All files have been updated with the new calculations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average_speed ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file: /workspaces/PRCDataChallenge/data/challenge_set_preproc.csv with new columns.\n",
      "Updated file: /workspaces/PRCDataChallenge/data/submission_set_preproc.csv with new columns.\n",
      "All files have been updated with new columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [os.path.join(source_data_folder,challenge_file_preproc), os.path.join(source_data_folder,submission_file_preproc)]\n",
    "\n",
    "# Function to calculate new columns based on time data\n",
    "def calculate_takeoff_time_duration(df):\n",
    "    # 1. Calculate takeoff_time (actual_offblock_time + taxiout_time)\n",
    "    df['takeoff_time'] = df['actual_offblock_time'] + pd.to_timedelta(df['taxiout_time'], unit='m')\n",
    "\n",
    "    # 2. Calculate duration in minutes (arrival_time - takeoff_time)\n",
    "    df['duration'] = (df['arrival_time'] - df['takeoff_time']).dt.total_seconds() / 60  # Convert to minutes\n",
    "\n",
    "    # 3. Calculate average_speed (flown_distance / duration)\n",
    "    df['average_speed'] = (df['flown_distance'] / df['duration']) * 60  # Convert to nautical miles per hour\n",
    "\n",
    "    df['average_speed'] = df['average_speed'] ** 2\n",
    "\n",
    "    return df\n",
    "\n",
    "# Loop through each file\n",
    "for file in files_to_process:\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Convert date columns to datetime format\n",
    "    df['actual_offblock_time'] = pd.to_datetime(df['actual_offblock_time'])\n",
    "    df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n",
    "\n",
    "    # Apply the calculations to the DataFrame\n",
    "    df = calculate_takeoff_time_duration(df)\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(file, index=False)\n",
    "\n",
    "    print(f\"Updated file: {file} with new columns.\")\n",
    "\n",
    "print(\"All files have been updated with new columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage offblock time and arrival time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The season, month, day, and hour columns have been added and saved to both files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [os.path.join(source_data_folder,challenge_file_preproc), os.path.join(source_data_folder,submission_file_preproc)]\n",
    "\n",
    "# Function to determine the season based on the month\n",
    "def determine_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 1  # Winter\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 2  # Spring\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 3  # Summer\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 4  # Autumn\n",
    "\n",
    "# Loop through each file\n",
    "for file in files_to_process:\n",
    "    # Read the file\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Ensure the 'actual_offblock_time' and 'arrival_time' columns are in datetime format\n",
    "    df['actual_offblock_time'] = pd.to_datetime(df['actual_offblock_time'])\n",
    "    df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n",
    "\n",
    "    # Extract month, day, and hour from actual_offblock_time\n",
    "    df['month_offblock_time'] = df['actual_offblock_time'].dt.month\n",
    "    df['day_offblock_time'] = df['actual_offblock_time'].dt.day\n",
    "    df['hour_offblock_time'] = df['actual_offblock_time'].dt.hour\n",
    "\n",
    "    # Extract month, day, and hour from arrival_time\n",
    "    df['month_arrival_time'] = df['arrival_time'].dt.month\n",
    "    df['day_arrival_time'] = df['arrival_time'].dt.day\n",
    "    df['hour_arrival_time'] = df['arrival_time'].dt.hour\n",
    "\n",
    "    # Apply the function to create the 'season' column\n",
    "    df['season'] = df['month_offblock_time'].apply(determine_season)\n",
    "\n",
    "    # Save the modified DataFrame back to the same file\n",
    "    df.to_csv(file, index=False)\n",
    "\n",
    "print(\"The season, month, day, and hour columns have been added and saved to both files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file: /workspaces/PRCDataChallenge/data/challenge_set_preproc.csv\n",
      "Updated file: /workspaces/PRCDataChallenge/data/submission_set_preproc.csv\n",
      "The 'hour_offblock_time' and 'day_period' columns have been added and saved to both files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [os.path.join(source_data_folder,challenge_file_preproc), os.path.join(source_data_folder,submission_file_preproc)]\n",
    "\n",
    "# Loop through each file\n",
    "for file in files_to_process:\n",
    "    # Read the data\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Ensure the 'actual_offblock_time' column is in datetime format\n",
    "    df['actual_offblock_time'] = pd.to_datetime(df['actual_offblock_time'])\n",
    "\n",
    "    # Extract the hour from the 'actual_offblock_time' column\n",
    "    df['hour_offblock_time'] = df['actual_offblock_time'].dt.hour\n",
    "\n",
    "    # Define a function to assign the time of day period\n",
    "    def determine_day_period(hour):\n",
    "        if hour < 9:\n",
    "            return 1  # Before 9 AM\n",
    "        elif 9 <= hour <= 16:\n",
    "            return 2  # Between 9 AM and 4 PM\n",
    "        else:\n",
    "            return 3  # After 4 PM\n",
    "\n",
    "    # Apply this function to create the 'day_period' column\n",
    "    df['day_period'] = df['hour_offblock_time'].apply(determine_day_period)\n",
    "\n",
    "    # Save the file with the new column\n",
    "    df.to_csv(file, index=False)\n",
    "\n",
    "    print(f\"Updated file: {file}\")\n",
    "\n",
    "print(\"The 'hour_offblock_time' and 'day_period' columns have been added and saved to both files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aircraft range category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file: /workspaces/PRCDataChallenge/data/challenge_set_preproc.csv\n",
      "Updated file: /workspaces/PRCDataChallenge/data/submission_set_preproc.csv\n",
      "The 'range_category' and 'range_category_encoded' columns have been added and saved to both files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [os.path.join(source_data_folder,challenge_file_preproc), os.path.join(source_data_folder,submission_file_preproc)]\n",
    "\n",
    "# Define a function to assign distance intervals\n",
    "def get_flown_distance_interval(distance):\n",
    "    if distance < 500:\n",
    "        return \"0-500\"\n",
    "    elif 500 <= distance < 1000:\n",
    "        return \"500-1000\"\n",
    "    elif 1000 <= distance < 1500:\n",
    "        return \"1000-1500\"\n",
    "    elif 1500 <= distance < 2000:\n",
    "        return \"1500-2000\"\n",
    "    elif 2000 <= distance < 4000:\n",
    "        return \"2000-4000\"\n",
    "    else:\n",
    "        return \"4000+\"\n",
    "\n",
    "# Loop through each file\n",
    "for file in files_to_process:\n",
    "    # Read the flight data\n",
    "    df_flight = pd.read_csv(file)\n",
    "\n",
    "    # Apply the function to create the flown_distance_interval column\n",
    "    df_flight['range_category'] = df_flight['flown_distance'].apply(get_flown_distance_interval)\n",
    "\n",
    "    # Use LabelEncoder to create the range_category_encoded column\n",
    "    le = LabelEncoder()\n",
    "    df_flight['range_category_encoded'] = le.fit_transform(df_flight['range_category'])\n",
    "\n",
    "    # Save the modified DataFrame back to the same file\n",
    "    df_flight.to_csv(file, index=False)\n",
    "\n",
    "    print(f\"Updated file: {file}\")\n",
    "\n",
    "print(\"The 'range_category' and 'range_category_encoded' columns have been added and saved to both files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AOBT seconds, days with sinus/cosinus\n",
    "# Speed and wind angle sinus/cosinus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: /workspaces/PRCDataChallenge/data/challenge_set_preproc.csv\n",
      "Processed and saved: /workspaces/PRCDataChallenge/data/submission_set_preproc.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [os.path.join(source_data_folder,challenge_file_preproc), os.path.join(source_data_folder,submission_file_preproc)]\n",
    "\n",
    "# Loop through each file\n",
    "for file in files_to_process:\n",
    "    # Load the flight data from the CSV file into a DataFrame\n",
    "    df_flight = pd.read_csv(file)\n",
    "\n",
    "    # Convert the 'actual_offblock_time' column to datetime format\n",
    "    df_flight['actual_offblock_time_dt'] = pd.to_datetime(df_flight['actual_offblock_time'])\n",
    "\n",
    "    # Calculate the number of seconds since the start of the day for 'actual_offblock_time'\n",
    "    df_flight['aobt_seconds'] = (df_flight['actual_offblock_time_dt'] - df_flight['actual_offblock_time_dt'].dt.normalize()) / pd.Timedelta('1 second')\n",
    "\n",
    "    # Compute the cosine of the seconds since the start of the day for cyclic feature representation\n",
    "    df_flight['aobt_seconds_cos'] = np.cos(df_flight['aobt_seconds'] * (np.pi * 2 / (3600 * 24)))\n",
    "\n",
    "    # Compute the sine of the seconds since the start of the day for cyclic feature representation\n",
    "    df_flight['aobt_seconds_sin'] = np.sin(df_flight['aobt_seconds'] * (np.pi * 2 / (3600 * 24)))\n",
    "\n",
    "    # Calculate the number of days since a reference date (January 1, 2022) for 'actual_offblock_time'\n",
    "    df_flight['aobt_days'] = (df_flight['actual_offblock_time_dt'] - pd.to_datetime(\"2022-01-01 00:00+00\")) / pd.Timedelta('1 day')\n",
    "\n",
    "    # Compute the cosine of the days since the reference date for cyclic feature representation\n",
    "    df_flight['aobt_days_cos'] = np.cos(df_flight['aobt_days'] * (np.pi * 2 / (365)))\n",
    "\n",
    "    # Compute the sine of the days since the reference date for cyclic feature representation\n",
    "    df_flight['aobt_days_sin'] = np.sin(df_flight['aobt_days'] * (np.pi * 2 / (365)))\n",
    "\n",
    "    df_flight[\"wind_track_angle_cos\"]=np.cos((np.pi/180)*(df_flight[\"ground_wind_direction\"]-df_flight[\"ground_airspeed_angle\"]))\n",
    "    df_flight[\"wind_track_angle_sin\"]=np.sin((np.pi/180)*(df_flight[\"ground_wind_direction\"]-df_flight[\"ground_airspeed_angle\"]))\n",
    "\n",
    "    df_flight[\"plateau_wind_track_angle_cos\"]=np.cos((np.pi/180)*(df_flight[\"plateau_wind_direction\"]-df_flight[\"plateau_airspeed_angle\"]))\n",
    "    df_flight[\"plateau_wind_track_angle_sin\"]=np.sin((np.pi/180)*(df_flight[\"plateau_wind_direction\"]-df_flight[\"plateau_airspeed_angle\"]))\n",
    "\n",
    "    # Save the modified DataFrame back to the same CSV file\n",
    "    df_flight.to_csv(file, index=False)\n",
    "\n",
    "    print(f\"Processed and saved: {file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add ICAO code, and ICAO couple\n",
    "IGNORE - REQUIRES DATA NOT MADE AVAILABLE (COULD NOT BE OPEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated files have been saved with the columns 'country_code_adep_icao' and 'country_code_ades_icao'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the files\n",
    "utc_file =  os.path.join(source_data_folder,os.getenv(\"UTC_OFFSET\"))\n",
    "\n",
    "file_1 = os.path.join(source_data_folder,challenge_file_preproc)\n",
    "file_2 = os.path.join(source_data_folder,submission_file_preproc)\n",
    "\n",
    "utc_df = pd.read_csv(utc_file)\n",
    "utc_df[\"ICAO_prefix\"]=utc_df['country_code_adep'].astype(str).str[0]\n",
    "df1 = pd.read_csv(file_1)\n",
    "df2 = pd.read_csv(file_2)\n",
    "\n",
    "# Merge with the 'UtcToLocalTimeOffset.csv' file to obtain 'ICAO_prefix' based on 'country_code_adep'\n",
    "df1_merged = pd.merge(df1, utc_df[['country_code_adep', 'ICAO_prefix']], left_on='country_code_adep', right_on='country_code_adep', how='left')\n",
    "df2_merged = pd.merge(df2, utc_df[['country_code_adep', 'ICAO_prefix']], left_on='country_code_adep', right_on='country_code_adep', how='left')\n",
    "\n",
    "# Rename the column 'ICAO_prefix' to 'country_code_adep_icao'\n",
    "df1_merged.rename(columns={'ICAO_prefix': 'country_code_adep_icao'}, inplace=True)\n",
    "df2_merged.rename(columns={'ICAO_prefix': 'country_code_adep_icao'}, inplace=True)\n",
    "\n",
    "# Add the column \"country_code_ades_icao\" based on \"ades\" and their corresponding ICAO code\n",
    "df1_merged = pd.merge(df1_merged, utc_df[['country_code_adep', 'ICAO_prefix']], left_on='country_code_ades', right_on='country_code_adep', how='left')\n",
    "df2_merged = pd.merge(df2_merged, utc_df[['country_code_adep', 'ICAO_prefix']], left_on='country_code_ades', right_on='country_code_adep', how='left')\n",
    "\n",
    "# Rename the second column 'ICAO_prefix' to 'country_code_ades_icao'\n",
    "df1_merged.rename(columns={'ICAO_prefix': 'country_code_ades_icao'}, inplace=True)\n",
    "df2_merged.rename(columns={'ICAO_prefix': 'country_code_ades_icao'}, inplace=True)\n",
    "\n",
    "# Remove duplicate columns after the merge\n",
    "df1_merged.drop(columns=['country_code_adep_y'], inplace=True)\n",
    "df2_merged.drop(columns=['country_code_adep_y'], inplace=True)\n",
    "\n",
    "# Save the updated files\n",
    "df1_merged.to_csv(file_1, index=False)\n",
    "df2_merged.to_csv(file_2, index=False)\n",
    "\n",
    "print(\"The updated files have been saved with the columns 'country_code_adep_icao' and 'country_code_ades_icao'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files have been updated with the 'country_code_icao_couple' column.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the cleaned files\n",
    "file_1 = os.path.join(source_data_folder,challenge_file_preproc)\n",
    "file_2 = os.path.join(source_data_folder,submission_file_preproc)\n",
    "\n",
    "df1 = pd.read_csv(file_1)\n",
    "df2 = pd.read_csv(file_2)\n",
    "\n",
    "# Function to generate the 'country_code_icao_couple' column\n",
    "def create_icao_couple(row):\n",
    "    adep_first_letter = str(row['country_code_adep_icao'])[0] if pd.notna(row['country_code_adep_icao']) else ''\n",
    "    ades_first_letter = str(row['country_code_ades_icao'])[0] if pd.notna(row['country_code_ades_icao']) else ''\n",
    "    \n",
    "    # Sort the letters in alphabetical order\n",
    "    sorted_letters = ''.join(sorted([adep_first_letter, ades_first_letter]))\n",
    "    return sorted_letters\n",
    "\n",
    "# Apply the function on both files\n",
    "df1['country_code_icao_couple'] = df1.apply(create_icao_couple, axis=1)\n",
    "df2['country_code_icao_couple'] = df2.apply(create_icao_couple, axis=1)\n",
    "\n",
    "# Save the files with the new column\n",
    "df1_merged.to_csv(file_1, index=False)\n",
    "df2_merged.to_csv(file_2, index=False)\n",
    "\n",
    "print(\"The files have been updated with the 'country_code_icao_couple' column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 14225, saw 110\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the two CSV files into DataFrames\u001b[39;00m\n\u001b[1;32m      4\u001b[0m airport_db \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/MyAirportDatabase_Updated.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m runways_db \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/runways.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Merge the two DataFrames on the 'airport' and 'airport_ident' columns\u001b[39;00m\n\u001b[1;32m      8\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(airport_db, runways_db[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairport_ident\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_ft\u001b[39m\u001b[38;5;124m'\u001b[39m]], \n\u001b[1;32m      9\u001b[0m                      left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairport\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairport_ident\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     10\u001b[0m                      how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, suffixes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_runway\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 14225, saw 110\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files into DataFrames\n",
    "airport_db = pd.read_csv('./data/MyAirportDatabase_Updated.csv')\n",
    "runways_db = pd.read_csv('./data/runways.csv')\n",
    "\n",
    "# Merge the two DataFrames on the 'airport' and 'airport_ident' columns\n",
    "merged_df = pd.merge(airport_db, runways_db[['airport_ident', 'length_ft']], \n",
    "                     left_on='airport', right_on='airport_ident', \n",
    "                     how='left', suffixes=('', '_runway'))\n",
    "\n",
    "# Fill the 'MinRWYlength' column with values from 'length_ft'\n",
    "merged_df['MinRWYlength'] = merged_df['length_ft']\n",
    "\n",
    "# Drop the intermediate 'length_ft' and 'airport_ident' columns if necessary\n",
    "merged_df.drop(columns=['length_ft', 'airport_ident'], inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "merged_df.to_csv('./database/MyAirportDatabase_Updated.csv', index=False)\n",
    "\n",
    "print(\"The 'MinRWYlength' column has been updated with runway lengths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'database'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     updated_airport_db[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaxi_in_seconds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m updated_airport_db[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTaxi_In\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Save the modified DataFrame back to the same CSV file\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mupdated_airport_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./database/MyAirportDatabase_Updated.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaxi_in_seconds\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column has been added successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'database'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the updated file into a DataFrame\n",
    "updated_airport_db = pd.read_csv('./data/MyAirportDatabase_Updated.csv')\n",
    "\n",
    "# Check if the 'Taxi_In' column exists\n",
    "if 'Taxi_In' in updated_airport_db.columns:\n",
    "    # Create the new column 'taxi_in_seconds'\n",
    "    updated_airport_db['taxi_in_seconds'] = updated_airport_db['Taxi_In'] * 60\n",
    "\n",
    "    # Save the modified DataFrame back to the same CSV file\n",
    "    updated_airport_db.to_csv('./data/MyAirportDatabase_Updated.csv', index=False)\n",
    "\n",
    "    print(\"The 'taxi_in_seconds' column has been added successfully.\")\n",
    "else:\n",
    "    print(\"The 'Taxi_In' column does not exist in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './database/MyAirportDatabase_Updated.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m final_submission \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(source_data_folder,submission_file_preproc)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the file with the 'taxi_in_seconds' column\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m airport_db \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./database/MyAirportDatabase_Updated.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Check if the 'taxi_in_seconds' column exists\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaxi_in_seconds\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m airport_db\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Remove duplicates while keeping the first occurrence of each airport\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './database/MyAirportDatabase_Updated.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files into DataFrames\n",
    "challenge_set = os.path.join(source_data_folder,challenge_file_preproc)\n",
    "final_submission = os.path.join(source_data_folder,submission_file_preproc)\n",
    "\n",
    "# Load the file with the 'taxi_in_seconds' column\n",
    "airport_db = pd.read_csv('./database/MyAirportDatabase_Updated.csv')\n",
    "\n",
    "# Check if the 'taxi_in_seconds' column exists\n",
    "if 'taxi_in_seconds' in airport_db.columns:\n",
    "    # Remove duplicates while keeping the first occurrence of each airport\n",
    "    airport_db_unique = airport_db[['airport', 'taxi_in_seconds']].drop_duplicates(subset='airport')\n",
    "\n",
    "    # Add taxi_in_seconds to the final_submission file\n",
    "    final_submission_merged = pd.merge(final_submission, airport_db_unique, \n",
    "                                        left_on='ades', right_on='airport', \n",
    "                                        how='left', suffixes=('', '_airport'))\n",
    "\n",
    "    # Add taxi_in_seconds to the challenge_set_test_2 file\n",
    "    challenge_set_merged = pd.merge(challenge_set, airport_db_unique, \n",
    "                                     left_on='ades', right_on='airport', \n",
    "                                     how='left', suffixes=('', '_airport'))\n",
    "\n",
    "    # Save the modified files\n",
    "    final_submission_merged.to_csv(os.path.join(source_data_folder,submission_file_preproc), index=False)\n",
    "    challenge_set_merged.to_csv(os.path.join(source_data_folder,challenge_file_preproc), index=False)\n",
    "\n",
    "    print(\"The 'taxi_in_seconds' columns have been added to the files successfully.\")\n",
    "else:\n",
    "    print(\"The 'taxi_in_seconds' column does not exist in the airport DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Default value if no condition is met\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Apply the function to create the taxi_in column\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m final_submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaxi_in\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_submission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m(calculate_taxi_in, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m challenge_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaxi_in\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m challenge_set\u001b[38;5;241m.\u001b[39mapply(calculate_taxi_in, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save the modified files\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files into DataFrames\n",
    "# Loading the cleaned files\n",
    "final_challenge = os.path.join(source_data_folder,challenge_file_preproc)\n",
    "final_submission = os.path.join(source_data_folder,submission_file_preproc)\n",
    "\n",
    "# Define a function to calculate taxi_in\n",
    "def calculate_taxi_in(row):\n",
    "    if row['aircraft_category'] == 'Heavy':\n",
    "        return row['taxi_in_seconds'] * 0.3\n",
    "    elif row['aircraft_category'] == 'Medium':\n",
    "        return row['taxi_in_seconds'] * 0.2\n",
    "    elif row['aircraft_category'] == 'Light':\n",
    "        return row['taxi_in_seconds'] * 0.1\n",
    "    else:\n",
    "        return None  # Default value if no condition is met\n",
    "\n",
    "# Apply the function to create the taxi_in column\n",
    "final_submission['taxi_in'] = final_submission.apply(calculate_taxi_in, axis=1)\n",
    "challenge_set['taxi_in'] = challenge_set.apply(calculate_taxi_in, axis=1)\n",
    "\n",
    "# Save the modified files\n",
    "final_submission.to_csv(os.path.join(source_data_folder,challenge_file_preproc), index=False)\n",
    "challenge_set.to_csv(os.path.join(source_data_folder,submission_file_preproc), index=False)\n",
    "\n",
    "print(\"The 'taxi_in' column has been added successfully to the files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runway_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files into DataFrames\n",
    "final_submission = pd.read_csv('./final_submission_set.csv')  # Use the updated file\n",
    "challenge_set = pd.read_csv('./challenge_set_test_2.csv')\n",
    "\n",
    "# Load the file with MinRWYlength to get the runway lengths\n",
    "airport_db = pd.read_csv('./database/MyAirportDatabase_Updated.csv')\n",
    "\n",
    "# Extract the first non-null value of MinRWYlength for each airport\n",
    "min_rwy_length_mapping = airport_db.groupby('airport')['MinRWYlength'].first().dropna()\n",
    "\n",
    "# Convert runway length from feet to kilometers\n",
    "min_rwy_length_mapping_km = min_rwy_length_mapping * 0.0003048  # 1 foot = 0.0003048 km\n",
    "\n",
    "# Create a new column runway_length in the DataFrames using the mapping\n",
    "final_submission['runway_length'] = final_submission['ades'].map(min_rwy_length_mapping_km)\n",
    "challenge_set['runway_length'] = challenge_set['ades'].map(min_rwy_length_mapping_km)\n",
    "\n",
    "# Save the modified files\n",
    "final_submission.to_csv('./final_submission_set_with_runway_length.csv', index=False)\n",
    "challenge_set.to_csv('./challenge_set_test_2_with_runway_length.csv', index=False)\n",
    "\n",
    "print(\"The 'runway_length' column has been added successfully to the files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low_cost Airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les fichiers\n",
    "final_submission = pd.read_csv(\"./final_submission_set.csv\")\n",
    "challenge_set_test = pd.read_csv(\"./challenge_set_test_2.csv\")\n",
    "low_cost_airports = pd.read_csv(\"./database/low-cost_Airports_Europe.csv\")\n",
    "\n",
    "# Obtenir la liste des codes ICAO des aéroports à bas coûts\n",
    "low_cost_icao_codes = set(low_cost_airports['ICAO Code'].dropna())\n",
    "\n",
    "# Fonction pour attribuer 1 ou 0 à low_cost_airport\n",
    "def assign_low_cost_airport(row):\n",
    "    if row['adep'] in low_cost_icao_codes or row['ades'] in low_cost_icao_codes:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Appliquer la fonction à chaque fichier\n",
    "final_submission['low_cost_airport'] = final_submission.apply(assign_low_cost_airport, axis=1)\n",
    "challenge_set_test['low_cost_airport'] = challenge_set_test.apply(assign_low_cost_airport, axis=1)\n",
    "\n",
    "# Enregistrer les fichiers modifiés\n",
    "final_submission.to_csv(\"./final_submission_set_1.csv\", index=False)\n",
    "challenge_set_test.to_csv(\"./challenge_set_test_2_1.csv\", index=False)\n",
    "\n",
    "print(\"Colonnes 'low_cost_airport' ajoutées et fichiers sauvegardés.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
